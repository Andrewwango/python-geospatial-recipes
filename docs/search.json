[
  {
    "objectID": "00_sample.html",
    "href": "00_sample.html",
    "title": "Notebooks",
    "section": "",
    "text": "In this notebook, we will demonstrate xxx using xxx dataset.\n\n\n\nimport os\n\n\n\n\n\nimport geopandas\n\n\n\n\n\nimport folium\n\n\nfolium.Map()\n\nMake this Notebook Trusted to load map: File -> Trust Notebook"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Processing Geospatial Data at Scale",
    "section": "",
    "text": "Demo for Processing Geospatial Data at Scale as part of Data Engineering Community of Practice Research Initiative."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Processing Geospatial Data at Scale",
    "section": "Introduction",
    "text": "Introduction\nWe demonstrate a number of geospatial processing tasks with larger datasets (click on link to view demo):\n\nVector table join - joining vector and table\nVector spatial join - map matching by nearest vectors\nSpatial vector aggregation - aggregate vectors by polygons\nRaster aggregation - calculate zonal statistics\nRaster resampling - interpolation\nGeospatial classification\n\nFor each task we show:\n\nData loading in Databricks\nData processing\nMap visualisation to HTML"
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "Processing Geospatial Data at Scale",
    "section": "Overview",
    "text": "Overview\n\n1. Vector table join\nHere, you join a spatial table with vector geometry with tabular data (example).\n\nTask: Plot a map of number of total internet users per UK authority.\nDataset: internet users by NUTS region, UK authority polygon shapefiles and NUTS code conversions.\nTech stack: geopandas and folium\nNotebook: 01_Table_Join.ipynb\n\n\n\n2. Vector spatial join\nHere, you add a column to a vector layer from another vector layer by joining on nearest geometries (example).\n\nTask: For given locations, show attributes of the nearest road on the UK road network.\nDataset: Ordnance Survey Open Roads\nTech stack: geopandas, shapely and folium\nNotebook: 02_Spatial_Join.ipynb\n\n\n\n3. Spatial vector aggregation\nHere, you aggregate a vector layer (e.g. of points or lines) by another vector layer of polygons. This is another form of vector spatial join.\n\nTask: Show a map of total length of road network per London borough.\nDataset: UK authority polygon shapefiles and Ordnance Survey Open Roads\nTech stack: geopandas, shapely and folium\nNotebook:\n\n\n\n4. Raster aggregation\nHere, you aggregate a raster layer (such as pixels such as images or gridded data such as elevation data) over polygons (example).\n\nTask:\nDataset:\nTech stack: rasterframe\nNotebook:\n\n\n\n5. Raster resampling/interpolation\nHere, you resample a raster at locations determined by a new grid, which may require interpolation of the original raster grid.\n\nTask: Resample raster of UK population to a new grid defined at a higher (~2.5x) resolution.\nDataset: UK gridded 2000 population density data from CIESIN\nTech stack: rasterio, scipy and folium\nNotebook: 05_Raster_Resampling.ipynb\n\n\n\n6. Geospatial classification\n\nTask:\nDataset:\nTech stack:\nNotebook:"
  },
  {
    "objectID": "05_Raster_Resampling.html",
    "href": "05_Raster_Resampling.html",
    "title": "Notebooks",
    "section": "",
    "text": "In this notebook, we will demonstrate resampling i.e. interpolating a raster dataset to a given different coordinate grid at a different resolution, with scipy. We will plot the raster layer using folium.\nThis is useful when you are doing a spatial join on 2 rasters which are defined on different grids - you will need to resample one raster onto the other’s grid.\nDatasets used:\n\nUK gridded 2000 population density data from CIESIN, downloaded for the UK from DIVA-GIS. Converted to geotiff using QGIS. Data resolution at roughly ~2.5km. The raster origin is at lon,lat=-8.266667,60.93333 (top left, EPSG:4326) and the grid is 458x500.\n\n\n\n\n!pip install numpy folium rasterio scipy\n\n\n\n\nFirst connect Databricks to your datalake.\n\ndatalake = \"/dbfs/mnt/copgeospatial\"\n\n\nimport numpy as np\nimport rasterio\n\n\ndataset = rasterio.open(f\"{datalake}/gbr_pop.tif\")\n\n\nbounds = dataset.bounds\n# BoundingBox(left=-8.266667, bottom=49.899995, right=1.8416677999999997, top=60.93333)\n\n\ndataset.shape\n# (500, 458)\n\n\npopulation = dataset.read(1)\n\n\n\n\nGet original grid coordinates\n\norig_mgrid = np.meshgrid(*map(np.arange, dataset.shape))\n\norig_grid = np.vstack(tuple(map(\n    np.ravel, \n    map(\n        np.array, \n        rasterio.transform.xy(dataset.transform, *orig_mgrid)\n    )\n))).T\n\norig_grid.shape\n# (229000, 2)\n\nDefine a completely new grid to resample onto, with resolution of thirty seconds (~1km):\n\nTHIRTY_SECONDS = 0.0083333333333333\n\nnew_mgrid = np.mgrid[bounds.left:bounds.right:THIRTY_SECONDS, bounds.bottom:bounds.top:THIRTY_SECONDS]\n\nnew_grid = np.vstack(tuple(map(\n    np.ravel,\n    new_mgrid\n))).T\n\nnew_grid.shape\n# (1608550, 2)\n\nResample raster onto new grid using scipy\n\nfrom scipy.interpolate import griddata\n\n\npopulation_resampled = griddata(\n    orig_grid, \n    np.where(population == -9999, np.nan, population).ravel(order='F'), # column-first ravel\n    new_grid,\n    method='linear'\n)\n\n\npopulation_resampled = np.nan_to_num(\n    population_resampled.reshape(*new_mgrid.shape[1:]), \n    nan=-9999\n)\n\n\n\n\nWe use a custom colormap to deal with nodata values. We also must use mercator_project=True to reproject our raster into the map’s underlying CRS.\n\nimport folium\ndef colormap_func(maxi, nodata=-9999):\n    return lambda x: (1,0,0,0 if x == nodata else max(0, x / maxi))\n\n\nm = folium.Map(location=[54.44, -3.5], zoom_start=12, tiles=\"CartoDB positron\")\n\nfolium.raster_layers.ImageOverlay(\n    population,\n    ((bounds.bottom, bounds.left), (bounds.top, bounds.right)),\n    colormap=colormap_func(population.max()),\n    mercator_project=True\n).add_to(m)\n\nfolium.LayerControl().add_to(m)\nm.fit_bounds(m.get_bounds()) \n\n\nm1 = folium.Map(location=[54.44, -3.5], zoom_start=12, tiles=\"CartoDB positron\")\n\nfolium.raster_layers.ImageOverlay(\n    np.flipud(population_resampled.T),\n    ((bounds.bottom, bounds.left), (bounds.top, bounds.right)),\n    colormap=colormap_func(population_resampled.max()),\n    mercator_project=True\n).add_to(m1)\n\nfolium.LayerControl().add_to(m1)\nm1.fit_bounds(m1.get_bounds()) \n\n\nm.save('/dbfs/mnt/copgeospatial/05_output.html')\nm1.save('/dbfs/mnt/copgeospatial/05_output_1.html')\n\n\n\nOriginal raster of population on ~2.5km grid\n\nfrom IPython.display import IFrame\nIFrame(\"05_output.html\", width=\"100%\", height=\"500\")\n\n\n        \n        \n\n\nResampled raster on ~1km grid\n\nIFrame(\"05_output_1.html\", width=\"100%\", height=\"500\")"
  },
  {
    "objectID": "02_Spatial_Join.html",
    "href": "02_Spatial_Join.html",
    "title": "Notebooks",
    "section": "",
    "text": "In this notebook, we will demonstrate joining a vector layer of points to another vector layer containing a road network using nearest neighbours.\nIn many transport logistics problems, locations to be visited first need to be snapped to the road network. Here, a power company needs to find the closest road to each power substation in order to find a route in the road network.\nDatasets used:\n\nOrdnance Survey Open Roads - road links filtered to the SP 100x100km grid square 51.724598,-2.001086,52.58593,-0.52677 roughly including half of Birmingham, Coventry, Oxford and Milton Keynes. Converted to EPSG:4326 and to geojson resulting in ~183 MB.\nOpenStreetMap power substations in SP grid downloaded using the API using the query node[power=substation](51.724598,-2.001086,52.58593,-0.52677);out;. < 1 MB\n\n\n\n\n!pip install geopandas folium\n\n\n\n\nFirst connect Databricks to your datalake.\n\ndatalake = \"/dbfs/mnt/copgeospatial\"\n\n\nimport pandas as pd \nimport geopandas as gpd\n\nRoads dataset geodataframe. Geometry is LineString, 255946 rows. Columns include gml_id (primary key), along with road details such as roadClassification, roadFunction, formOfWay, length.\n\ngdf_roads = gpd.read_file(f\"{datalake}/OS_OpenRoads_SP_RoadLink_4326.geojson\")\ngdf_roads.head()\n\nPower substations geodataframe. Geometry is Point, 1238 rows. Columns include id (primary key), and OpenStreetMap tags for substations such as name, operator, voltage etc.\n\ngdf_substations = gpd.read_file(f\"{datalake}/SP_Power_Substations.geojson\")\ngdf_substations.head()\n\n\n\n\nPerform a spatial join using native geopandas methods. Note that because the UK is small, we can approximately perform planar distance calculations despite both geodataframes being in an unprojected geographic CRS EPSG:4326. To do this, we can convert a distance in metres to a degree “distance” by dividing by R * pi / 180 where R=6371000 is the radius of the Earth.\n\ngdf_roads_substations = gdf_substations.sjoin_nearest(\n    gdf_roads, \n    how=\"left\",\n    max_distance=100/111195 #100 metres in units of degrees\n).merge(\n    gdf_roads, \n    on=\"gml_id\", \n    suffixes=(\"_delete\", None)\n)\n\n\n\n\n\nimport folium\n\n\nm = folium.Map(location=[54.44, -3.5], width=500, height=750, zoom_start=12, tiles=\"CartoDB positron\")\n\nm = gdf_roads_substations.explore(\n    m=m,\n    tooltip=[\"gml_id\", \"roadFunction\", \"formOfWay\", \"length\"]\n)\n\nfor i, row in gdf_substations.iterrows():\n    folium.Circle(\n        location=(row.geometry.y, row.geometry.x), \n        radius=30, \n        color=\"crimson\",\n        fill=True,\n        tooltip=f\"Substation name {row['name']}\"\n    ).add_to(m)\n\nfolium.LayerControl().add_to(m)\nm.fit_bounds(m.get_bounds()) \n\n\nm\n\n\nm.save('/dbfs/mnt/copgeospatial/02_output.html')\n\n\n\n\nfrom IPython.display import IFrame\nIFrame(\"02_output.html\", width=\"100%\", height=\"700\")"
  },
  {
    "objectID": "00_Connect_Datalake.html",
    "href": "00_Connect_Datalake.html",
    "title": "Notebooks",
    "section": "",
    "text": "Connect to datalake in Azure Storage Account\nRun this code to mount your Azure Storage Account to Databricks.\nThis assumes you have a storage account dbcopgeospatial. Remember to remove the storage account key before committing.\n::: {.cell application/vnd.databricks.v1+cell=‘{“cellMetadata”:{“byteLimit”:2048000,“rowLimit”:10000},“inputWidgets”:{},“nuid”:“82e8a9ba-8b39-4973-955f-d2bf802245e5”,“showTitle”:false,“title”:““}’ execution_count=0}\nimport os\n\n# Credentials\nSTORAGE_ACCOUNT_NAME = \"dbcopgeospatial\"\nSTORAGE_ACCOUNT_KEY = \"\"\n\n# Paths\nPATH_EXTERNAL_BLOB = f\"wasbs://external@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\"\nPATH_DATALAKE = f\"wasbs://copgeospatial@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\"\nPATH_ANALYSIS = f\"wasbs://analysis@{STORAGE_ACCOUNT_NAME}.blob.core.windows.net/\"\n\n# Connect blob\nspark.conf.set(\n    f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\",\n    STORAGE_ACCOUNT_KEY,\n)\n\n# Mounts\ndef mount(mount_name: str, source: str, mount_root: str = \"/mnt\") -> None:\n    \"\"\"\n    Mounts source location to local path under mount_local_root. Will ignore if mount\n    alreay exits.\n\n    Note: This function expects access to global STORAGE_ACCOUNT_NAME and\n    STORAGE_ACCOUNT_KEY constants.\n\n    Parameters:\n        - local_name: mounted local directory name (subdirectory of mount_local_root)\n        - source: location of source (e.g. blob container)\n        - mount_local_root (optional): the root path of the local mounts\n    \"\"\"\n    if f\"{mount_name}/\" not in [\n        fileinfo.name for fileinfo in dbutils.fs.ls(mount_root)\n    ]:\n        print(mount_name)\n        dbutils.fs.mount(\n            source=source,\n            mount_point=os.path.join(mount_root, mount_name),\n            extra_configs={\n                f\"fs.azure.account.key.{STORAGE_ACCOUNT_NAME}.blob.core.windows.net\": STORAGE_ACCOUNT_KEY\n            },\n        )\n\n\nfor name, container in zip(\n    [\"copgeospatial\"],\n    [PATH_DATALAKE],\n):\n    mount(name, container)\n:::"
  },
  {
    "objectID": "01_Table_Join.html",
    "href": "01_Table_Join.html",
    "title": "Notebooks",
    "section": "",
    "text": "In this notebook, we will demonstrate joining a table of internet users per NUTS area with UK local authority polygons.\nDatasets used:\n\nOffice of National Statistics Internet users by NUTS code\nNUTS Code conversions\nUK authority polygon shapefiles\n\n\n\n\n!pip install geopandas folium\n\n\n\n\nFirst connect Databricks to your datalake.\n\ndatalake = \"/dbfs/mnt/copgeospatial\"\n\n\nimport pandas as pd \nimport geopandas as gpd\n\nInternet users by NUTS code\n\ndf_internet = pd.read_csv(f\"{datalake}/internetusers2020_updt.csv\")\n\ndf_internet.columns = [\n    \"NUTS_Code\", \"Region_Name\", \n    \"2014\", \"2015\", \"2016\", \"2017\", \"2018\", \"2019\", \"2020\"\n]\ndf_internet.head()\n\nNUTS to local administrative unit conversion (columns LAU118CD, NUTS318CD):\n\ndf_nuts = pd.read_csv(f\"{datalake}/LAU2_to_LAU1_to_NUTS3_to_NUTS2_to_NUTS1_(December_2018)_Lookup_in_United_Kingdom.csv\") \\\n\ndf_nuts = df_nuts[[\"LAU118CD\", \"NUTS318CD\"]].drop_duplicates()\ndf_nuts.sort_values(by=\"NUTS318CD\").head()\n\nUK region shapefiles stored in a geodataframe (with geometry column):\n\ngdf = gpd.read_file(f\"{datalake}/Counties_and_Unitary_Authorities_(December_2022)_UK_BFC/Counties_and_Unitary_Authorities_(December_2022)_UK_BFC.shp\")\ngdf.head()\n\n\n\n\nMerge 3 datasets: local authority vector polygons, NUTS codes and internet users per NUTS code:\n\ndf_merged = df_internet \\\n    .merge(df_nuts, left_on=\"NUTS_Code\", right_on=\"NUTS318CD\", how=\"left\") \\\n    .merge(gdf, left_on=\"LAU118CD\", right_on=\"CTYUA22CD\", how=\"right\")\n\ndf_merged = df_merged[[\n    'Region_Name', '2014', '2015', '2016', '2017', '2018', '2019', '2020',\n    'LAU118CD', 'Shape__Are', 'Shape__Len', 'GlobalID', 'geometry'\n]].astype(str)\n\nConvert the merged dataframe to a geodataframe:\n\nfrom shapely.wkt import loads\n\n\ndf_merged['GlobalID'] = df_merged['GlobalID'].apply(lambda x: str(x))\ndf_merged['2020']     = df_merged['2020'    ].apply(lambda x: float(x))\ndf_merged['geometry'] = df_merged['geometry'].apply(loads)\n\n\ngdf_merged = gpd.GeoDataFrame(df_merged, geometry='geometry').set_crs('epsg:27700')\n\nSimplify geometries for faster plotting and convert CRS for plotting on top of world maps:\n\ngdf_merged[\"geometry\"] = gdf_merged[\"geometry\"].simplify(tolerance=100, preserve_topology=False)\n\n\ngdf_merged = gdf_merged.to_crs(epsg=4326)\n\n\n\n\n\nimport folium\n\n\nm = folium.Map(location=[54.44, -3.5], width=500, height=750, zoom_start=12, tiles=\"CartoDB positron\")\n\nlayer = folium.Choropleth(\n    geo_data = gpd.GeoSeries(gdf_merged.set_index(\"GlobalID\")['geometry']).to_json(),\n    name=\"Choropleth\",\n    data=gdf_merged,\n    key_on='feature.id',\n    columns=['GlobalID', '2020'],\n    bins=[0, 50, 150, 250, 300, 550, 1000],\n    fill_color='YlGnBu',\n    fill_opacity=0.5,\n    line_opacity=1.0,\n    legend_name='Internet Users in millions').add_to(m)\n\nfolium.LayerControl().add_to(m)\nm.fit_bounds(layer.get_bounds()) \n\n\nm.save('/dbfs/mnt/copgeospatial/01_output.html')\n\n\n\n\nfrom IPython.display import IFrame\nIFrame(\"01_output.html\", width=\"100%\", height=\"700\")"
  }
]